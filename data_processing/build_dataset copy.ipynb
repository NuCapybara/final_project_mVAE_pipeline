{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P9BcgScZ_hYf"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"UKRnvFCt3sWI"},"source":["**This file is to build the final augmented dataset used for training model, including two major parts:**\n","\n","\n","**1. Post processing: singal post-processing for both raw data collected from human and robot.** \n","   1. For human: downsampling (decimate) both EMG/IMU to the rate at 10 Hz\n","   2. For robot: downsampling (decimate) one segment of datapoints to 8 points, add 2 static points at the end of each segment. Integrate 4 segments into single sequence. \n","   3. Concatenate 10 repeats for both human and robot data, and then align them to obtain the original dataset.\n","\n","**2. Data augmentation: apply augmentaion on original dataset in order to train the mVAE model**\n","   1. Using sklearn.preprocessing.MinMaxScaler to normailize the original data to the range at [-1, 1]\n","   2. Horizontally concatenate all data points at current time (at t) with them at previous time (at t -1)\n","   3. Split dataset into training set and testing set at ratio of 80:20\n","   4. Mask all robot data in training set with value -2 to obtain the case 2 dataset; mask all original data at t in training set with value -2  to obtain case 3 dataset\n","   5. Vertically concatenate original data with case 2 and case 3 data to obtain the final augmented training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OADh0S87_wkd"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd \n","import numpy as np\n","from scipy.signal import detrend\n","from scipy import signal\n","import math\n","from sklearn import preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siFYBSfs_xRL"},"outputs":[],"source":["########### Below is the part for processing emg data ###########\n","### function for getting emg data in one task for one arm \n","def process_emg(i, arm):  \n","  df_emg = pd.read_csv('/content/drive/MyDrive/finalProject/human_/task_'  + str(i) + '-' + arm +'_myo-emg.csv')\n","  #slice rows : 10 trials (40s) in total\n","  df_emg = df_emg.iloc[200:8200,:]\n","\n","  # convert '.data' col to array\n","  df_emg['.data'] = df_emg['.data'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n","  def f(emgs): \n","      return [emg for i, emg in enumerate(emgs)]\n","\n","  emg_names = ['emg_0', 'emg_1', 'emg_2', 'emg_3', 'emg_4', 'emg_5', 'emg_6', 'emg_7']\n","  df_emg[emg_names] = df_emg.apply(lambda x: f(x['.data']), axis=1, result_type='expand')\n","\n","  # time_elapsed\n","  df_emg['time']=pd.to_datetime(df_emg['time'])\n","  df_emg['time_elapsed']=df_emg['time'].apply(lambda t : t - df_emg.iloc[0,0]).dt.total_seconds()\n","\n","  #post_process \n","  df_emg_proc = pd.DataFrame()\n","  for emg in emg_names:\n","    df_emg[emg] = detrend(df_emg[emg], type='constant')       ##detrend: remove any constant offset\n","    df_emg[emg] = abs(df_emg[emg])                            ##Full-wave rectification: simply just take the absolute values \n","    df_emg[emg] = df_emg[emg].rolling(window=50, min_periods=1).mean()       ##Smoothening: moving average with 250ms sliding window\n","    #downsample to 10hz  (400points for 40s)\n","    q = 20 # downsampling factor\n","    df_emg_proc[emg] = signal.decimate(df_emg[emg], q)\n","  duration = df_emg['time_elapsed'].tail(1).item()\n","  df_emg_proc['time_elapsed']= np.linspace(0, duration, len(df_emg_proc))\n","\n","  return df_emg_proc\n","#################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-0zvOf2ErvR"},"outputs":[],"source":["########### Below is the part for processing imu data ###########\n","### function for getting imu data in one task for one arm \n","def process_imu(i, arm):\n","  df_imu = pd.read_csv('/content/drive/MyDrive/finalProject/human_/task_'  + str(i) + '-' + arm +'_myo-imu.csv')\n","  df_imu = df_imu.iloc[50:2050,:]\n","\n","  # time_elapsed\n","  df_imu['time']=pd.to_datetime(df_imu['time'])\n","  df_imu['time_elapsed']=df_imu['time'].apply(lambda t : t - df_imu.iloc[0,0]).dt.total_seconds()\n","\n","  # post_process\n","  df_imu_proc = pd.DataFrame()\n","  #downsample to 10hz  (400points for 40s)\n","  q = 5 # downsampling factor\n","  #for ori\n","  ori_name = ['.orientation.x', '.orientation.y', '.orientation.z', '.orientation.w']\n","  for ori in ori_name:\n","    df_imu_proc[ori] = signal.decimate(df_imu[ori], q)\n","  #for ang_vel\n","  ang_vel_name = ['.angular_velocity.x', '.angular_velocity.y', '.angular_velocity.z']\n","  for ang_vel in ang_vel_name:\n","    df_imu_proc[ang_vel] = signal.decimate(df_imu[ang_vel], q)\n","  #for lin_acc\n","  lin_acc_name = ['.linear_acceleration.x', '.linear_acceleration.y', '.linear_acceleration.z']\n","  for lin_acc in lin_acc_name:\n","    df_imu_proc[lin_acc] = signal.decimate(df_imu[lin_acc], q)\n","\n","  duration = df_imu['time_elapsed'].tail(1).item()\n","  df_imu_proc['time_elapsed']= np.linspace(0, duration, len(df_imu_proc))\n","\n","  return df_imu_proc\n","###################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQqg5TkRnyVg"},"outputs":[],"source":["########### Below is the part for processing robot data ###########\n","### function for getting robo data segment: 4 seg in one sequence\n","def build_robo_segment(task, trial, seg):\n","  df_robo = pd.read_csv('/content/drive/MyDrive/finalProject/robot_/' + str(task) + '/trial_' + str(trial) + '_' + str(seg) + '-points_data.csv')\n","  # convert '.data' col to array\n","  df_robo['.position'] = df_robo['.position'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n","  def split(positions): \n","      return [pos for i, pos in enumerate(positions)]\n","\n","  pos_names = ['pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7', 'f_pos_1', 'f_pos_2']\n","  df_robo[pos_names] = df_robo.apply(lambda x: split(x['.position']), axis=1, result_type='expand')\n","  #extract joint position data\n","  joints_pos_names = pos_names[:7]\n","  df_robo_pos = df_robo[joints_pos_names]\n","  df_robo_pos['real_time'] = [i / 10 for i in range(len(df_robo_pos))]\n","  #process robo data (decimate and add noise)\n","  df_robo_proc = pd.DataFrame()\n","  for pos in joints_pos_names:\n","    q = math.ceil(len(df_robo_pos)/8) # downsampling factor -> resample to size of 8\n","    df_robo_proc[pos] = signal.decimate(df_robo_pos[pos], q, n=3) # n is the order of filter\n","    # df_robo_proc[pos] += np.random.normal(0,0.02,len(df_robo_proc)) # add G noise\n","  duration = df_robo_pos['real_time'].tail(1).item()\n","  df_robo_proc['proc_time'] = np.linspace(0, duration, len(df_robo_proc))\n","  ## add more static points in th end\n","  end_pos = df_robo_proc.tail(1)\n","  length = len(df_robo_proc)\n","  while length < 10:\n","    df_robo_proc = df_robo_proc.append(end_pos, ignore_index = True)\n","    length += 1\n","  \n","  return df_robo_proc  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8fAor289FU6"},"outputs":[],"source":["### function for building integrated robo data: pos and vel for whole single trial\n","def build_robo_integrated(task, trial):\n","  df_robo_seg_list = []\n","  for j in range(4):\n","    df_robo_seg_list.append(build_robo_segment(task, trial, seg=j + 1))\n","  ##integrate one trial\n","  df_robo_integrated = pd.concat(df_robo_seg_list, ignore_index = True)\n","\n","  ### fabricate time stamp and generate vel\n","  df_robo_integrated['proc_time'] = np.linspace(0, 4, len(df_robo_integrated))\n","  joints_pos_names = ['pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7']\n","  joints_vel_names = ['vel_1', 'vel_2', 'vel_3', 'vel_4', 'vel_5', 'vel_6', 'vel_7']\n","  for pos, vel in zip(joints_pos_names, joints_vel_names):\n","    vel_list = [0] # start from 0 vel\n","    for i in range(len(df_robo_integrated) - 1):\n","      vel_instant = (df_robo_integrated[pos][i+1] - df_robo_integrated[pos][i]) / (df_robo_integrated['proc_time'][i+1] - df_robo_integrated['proc_time'][i])\n","      vel_list.append(vel_instant)\n","    df_robo_integrated[vel] = vel_list\n","\n","  return df_robo_integrated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaWww5UKEwY6"},"outputs":[],"source":["### function for getting repeats for 10 trials in one task\n","def process_robo(k):\n","  df_robo_repeat_list = []\n","  for i in range(10):\n","    df_robo_repeat_list.append(build_robo_integrated(task=k, trial=i+1))\n","    \n","  return pd.concat(df_robo_repeat_list, ignore_index=True)\n","########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8Xa2kHA2Dmu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","def get_original_dataset(radius, angle, height):\n","    # Base paths\n","    base_emg_path = \"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/emg_csv_data/emg_combined_sync_smooth_data\"\n","    base_robot_position_path = \"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/processed_robot_position_data\"\n","    base_robot_velocity_path = \"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/processed_robot_velocity_data\"\n","\n","    # Universal folder naming pattern\n","    folder_name = f\"r{radius}deg{angle}h{height}\"\n","\n","    # Construct full paths for EMG and IMU files\n","    emg_folder = os.path.join(base_emg_path, f\"H_{folder_name}_segmented\")\n","    position_file = f\"processed_R_{folder_name}_0.csv\"\n","    velocity_file = f\"processed_velocities_R_{folder_name}_0.csv\"\n","\n","    # Load EMG and IMU data\n","    RL_emg = pd.read_csv(os.path.join(emg_folder, \"RL_emg_combined.csv\")).drop(columns=['Timestamp'], errors='ignore')\n","    RL_imu = pd.read_csv(os.path.join(emg_folder, \"RL_imu_combined.csv\")).drop(columns=['Timestamp'], errors='ignore')\n","    RU_emg = pd.read_csv(os.path.join(emg_folder, \"RU_emg_combined.csv\")).drop(columns=['Timestamp'], errors='ignore')\n","    RU_imu = pd.read_csv(os.path.join(emg_folder, \"RU_imu_combined.csv\")).drop(columns=['Timestamp'], errors='ignore')\n","\n","    # Load robot position and velocity data\n","    robot_position = pd.read_csv(os.path.join(base_robot_position_path, position_file)).drop(columns=['Timestamp'], errors='ignore')\n","    robot_velocity = pd.read_csv(os.path.join(base_robot_velocity_path, velocity_file)).drop(columns=['Timestamp'], errors='ignore')\n","\n","    # Combine datasets column-wise\n","    combined_data = pd.concat([RL_imu, RL_emg, RU_imu, RU_emg, robot_position, robot_velocity], axis=1)\n","\n","    # Return combined dataset\n","    return combined_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUBKXqtp49Mt"},"outputs":[],"source":["# ### obtain the original dataset\n","# df_original_dataset = get_original_dataset(5)\n","# ###save as csv\n","# %cd /content/drive/MyDrive/finalProject\n","# df_original_dataset.to_csv('original_data_only_t.csv')\n","# ########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","# Define the range of parameters for radius, angle, and height\n","radii = [1, 2, 3]  # Example radii\n","angles = [0, 22, 45, 67, 90, 112, 135, 157, 180]  # Example angles\n","heights = [0, 1]  # Example heights\n","\n","def get_original_dataset():\n","    task_list = []  # To store all trials\n","\n","    # Loop through all combinations of parameters\n","    for radius in radii:\n","        for angle in angles:\n","            for height in heights:\n","                try:\n","                    # Generate the dataset for the current parameters\n","                    df_RL_emg = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/emg_csv_data/emg_combined_sync_smooth_data/H_r{radius}deg{angle}h{height}_segmented/RL_emg_combined.csv\").drop(columns=['Timestamp'], errors='ignore')\n","                    df_RL_imu = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/emg_csv_data/emg_combined_sync_smooth_data/H_r{radius}deg{angle}h{height}_segmented/RL_imu_combined.csv\").drop(columns=['Timestamp'], errors='ignore')\n","                    df_RU_emg = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/emg_csv_data/emg_combined_sync_smooth_data/H_r{radius}deg{angle}h{height}_segmented/RU_emg_combined.csv\").drop(columns=['Timestamp'], errors='ignore')\n","                    df_RU_imu = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/emg_csv_data/emg_combined_sync_smooth_data/H_r{radius}deg{angle}h{height}_segmented/RU_imu_combined.csv\").drop(columns=['Timestamp'], errors='ignore')\n","                    df_robo_position = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/processed_robot_position_data/processed_R_r{radius}deg{angle}h{height}_0.csv\").drop(columns=['Timestamp'], errors='ignore')\n","                    df_robo_velocity = pd.read_csv(f\"/home/jialuyu/Data_Final_Project/DataProcessingFinalProject/processed_robot_velocity_data/processed_velocities_R_r{radius}deg{angle}h{height}_0.csv\").drop(columns=['Timestamp'], errors='ignore')\n","\n","                    # Combine datasets for the current trial\n","                    combined_data = [df_RL_imu, df_RL_emg, df_RU_imu, df_RU_emg, df_robo_position, df_robo_velocity]\n","                    df_data_one_task = pd.concat(combined_data, axis=1)\n","\n","                    # Append to the task list\n","                    task_list.append(df_data_one_task)\n","                \n","                except Exception as e:\n","                    # Handle missing files or errors gracefully\n","                    print(f\"Skipping r{radius}deg{angle}h{height} due to error: {e}\")\n","\n","    # Combine all tasks into a single DataFrame\n","    df_original_data = pd.concat(task_list, ignore_index=True)\n","    return df_original_data\n","\n","# Obtain the original dataset\n","df_original_dataset = get_original_dataset()\n","\n","# Save the combined dataset as a single CSV\n","output_path = \"/home/jialuyu/Data_Final_Project/Revised_James_Code/final_project_mVAE_pipeline/data_processing/original_data_only_t.csv\"\n","df_original_dataset.to_csv(output_path, index=False)\n","print(f\"Combined dataset saved to {output_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-73jIYdH851"},"outputs":[],"source":["########### Below is the part for data augmentation ########\n","### normalize within -1 and 1 -> coln by coln \n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1)) \n","scaler.fit(df_original_dataset)\n","scaled = scaler.fit_transform(df_original_dataset)\n","scaled_original_dataset = pd.DataFrame(scaled, columns=df_original_dataset.columns)\n","scaled_original_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0uoPkpR8k8m"},"outputs":[],"source":["###combine t with t - 1\n","df_original_dataset_prev = scaled_original_dataset.iloc[:-1, :]\n","df_original_dataset_cur = scaled_original_dataset.iloc[1:, :]\n","dataset_list = [df_original_dataset_cur, df_original_dataset_prev]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAB42PGG7DOs"},"outputs":[],"source":["#[df_RL_imu, df_RL_emg, df_RU_imu, df_RU_emg, df_robo_pos, robo_vel]\n","### function for build a dataset with data at t and at t-1\n","def create_cur_prev_dataset(i):\n","  # 0: cur, 1: prev\n","  RL_imu = dataset_list[i].iloc[:,:10].reset_index(drop=True)\n","  RL_emg = dataset_list[i].iloc[:,10:18].reset_index(drop=True)\n","  RU_imu = dataset_list[i].iloc[:,18:28].reset_index(drop=True)\n","  RU_emg = dataset_list[i].iloc[:,28:36].reset_index(drop=True)\n","  robo_pos = dataset_list[i].iloc[:,36:43].reset_index(drop=True)\n","  robo_vel = dataset_list[i].iloc[:,43:50].reset_index(drop=True)\n","\n","  return [RL_imu, RL_emg, RU_imu, RU_emg, robo_pos, robo_vel]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5UsqCuLB0N-"},"outputs":[],"source":["cur_data_list = create_cur_prev_dataset(0)\n","prev_data_list = create_cur_prev_dataset(1)\n","\n","#pd.concat(combined_data, axis=1)\n","item_list = []\n","for i in range(len(cur_data_list)):\n","  item_list.append(cur_data_list[i])\n","  item_list.append(prev_data_list[i])\n","\n","data_with_cur_prev = pd.concat(item_list, axis=1)\n","data_with_cur_prev.to_csv('original_data_with_cur_prev.csv')  # 1999 rows × 100 columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58QSC-KiLCbE"},"outputs":[],"source":["####split dataset to train 80%, test 20%\n","from sklearn.model_selection import train_test_split\n","training_data, testing_data = train_test_split(data_with_cur_prev, test_size=0.2)\n","training_data.to_csv('raw_training_data.csv')\n","testing_data.to_csv('testing_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU5U6GLwcMdZ"},"outputs":[],"source":["### load training data\n","# case 1: original data\n","training_data = pd.read_csv('raw_training_data.csv', header=None, skiprows=1, index_col=[0]).reset_index(drop=True) # 1599 rows × 100 columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uhyVl9FMUbM"},"outputs":[],"source":["#### augment for training data\n","# case 2: mask robot data \n","\n","# training_data.reset_index(drop=True, inplace=True)\n","masked_robo = pd.DataFrame(np.full((training_data.shape[0],28),-2))\n","training_no_robo = pd.concat([training_data.iloc[:, :72],masked_robo], axis=1)\n","training_no_robo.columns = training_data.columns\n","training_no_robo.to_csv('training_no_robo.csv') # 1599 rows × 100 columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFFB0Qj1Tp00"},"outputs":[],"source":["# case 3: mask data at t\n","masked_imu_cur = pd.DataFrame(np.full((training_data.shape[0],10),-2))\n","masked_emg_cur = pd.DataFrame(np.full((training_data.shape[0],8),-2))\n","masked_robo_cur = pd.DataFrame(np.full((training_data.shape[0],7),-2))\n","aug_item_list = [masked_imu_cur, training_data.iloc[:,10:20], masked_emg_cur, training_data.iloc[:,28:36],\n","                 masked_imu_cur, training_data.iloc[:,46:56], masked_emg_cur, training_data.iloc[:,64:72],\n","                 masked_robo_cur, training_data.iloc[:,79:86], masked_robo_cur, training_data.iloc[:,93:100]]\n","training_no_cur = pd.concat(aug_item_list, axis=1)\n","training_no_cur.columns = training_data.columns\n","training_no_cur.to_csv('training_no_cur.csv') # 1599 rows × 100 columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwNE8578WsuB"},"outputs":[],"source":["##concat case 1, 2, 3\n","aug_training_list = [training_data, training_no_robo, training_no_cur]\n","aug_training_data = pd.concat(aug_training_list, axis=0, ignore_index=True)\n","aug_training_data # 4797 rows × 100 columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2ZCXJ0hfQu3"},"outputs":[],"source":["### aug + original\n","original_training_data_for_label = pd.concat([training_data,training_data,training_data], axis=0, ignore_index=True)\n","final_list = [aug_training_data, original_training_data_for_label]\n","final_aug_training = pd.concat(final_list, axis=1)\n","### save final aug training data\n","final_aug_training.to_csv('final_aug_training_data.csv') # 4797 rows × 200 columns"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMWdSCyjC8Jqa4sYo9NFQ8d","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
